{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6225 passages\n",
      "685 pages\n"
     ]
    }
   ],
   "source": [
    "# collect all questions per paragraph id\n",
    "from collections import defaultdict\n",
    "\n",
    "paras = []\n",
    "path = '../data/tasks15/test_paragraphs.tsv'\n",
    "qp_ids = defaultdict(list)\n",
    "p_ids = []\n",
    "\n",
    "with open(path, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        q_id, para_id, para_text = line.rstrip().split('\\t')\n",
    "        if q_id not in qp_ids[para_id]:\n",
    "            qp_ids[para_id].append(q_id)\n",
    "        if para_text not in paras:\n",
    "            paras.append(para_text)\n",
    "            p_ids.append(para_id)\n",
    "            \n",
    "# print(len(paras))\n",
    "print(len(p_ids), 'passages')\n",
    "print(len(qp_ids), 'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 pages with generated questions\n"
     ]
    }
   ],
   "source": [
    "# load all generated questions per passage\n",
    "import numpy as np\n",
    "\n",
    "qs_per_page = defaultdict(list)\n",
    "with open('../results/tasks15/t5_tasks15_test_paragraphs_13.tsv', 'r') as fin:\n",
    "    for i, line in enumerate(fin):\n",
    "        cells = line.rstrip('\\n').split('\\t')\n",
    "        # load generated questions\n",
    "        p_id = p_ids[i]\n",
    "        qs = cells[2:]\n",
    "        qs_per_page[p_id].extend(qs)\n",
    "print(len(qs_per_page), 'pages with generated questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547 questions\n"
     ]
    }
   ],
   "source": [
    "# load gt questions\n",
    "# collect all GS questions: independent\n",
    "path_qs = '../data/tasks15/test_questions.tsv'\n",
    "\n",
    "gs_qs = {}\n",
    "with open(path_qs, 'r') as f:\n",
    "#     next(f)\n",
    "    for line in f:\n",
    "        q_id, q = line.split('\\t')\n",
    "#         print(q_id)\n",
    "        gs_qs[q_id] = (q.strip('\\n'))\n",
    "print(len(gs_qs), 'questions')\n",
    "# print(gs_qs['1_11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 pages with generated questions\n",
      "0.36992105358364874\n",
      "0.41466532354531843\n",
      "0.3522205852388888\n",
      "0.39196121113967203\n",
      "134.65058236272878\n"
     ]
    }
   ],
   "source": [
    "# evaluate generated questions\n",
    "import numpy as np\n",
    "\n",
    "max_scores_1f, max_scores_1r = [], []\n",
    "max_scores_Lf, max_scores_Lr = [], []\n",
    "\n",
    "\n",
    "# iterate over pages\n",
    "print(len(qs_per_page), 'pages with generated questions')\n",
    "n_generated = []\n",
    "for page_id, qs in qs_per_page.items():\n",
    "        # iterate over gt questions for each page\n",
    "        q_ids = qp_ids[page_id]\n",
    "#         print(page_id)\n",
    "#         print(len(q_ids), 'gold truth questions for one page')\n",
    "#         print(q_ids)\n",
    "#         print(len(qs), 'generated questions for this page')\n",
    "        n_generated.append(len(qs))\n",
    "        for q_id in q_ids:\n",
    "            gt_q = gs_qs[q_id]\n",
    "            all_q_gens_scores_1f,  all_q_gens_scores_1r = [], []\n",
    "            all_q_gens_scores_Lf,  all_q_gens_scores_Lr = [], []\n",
    "            # iterate over generated questions for each page\n",
    "            for q in qs:\n",
    "                scores = scorer.score(gt_q, q)\n",
    "                all_q_gens_scores_1f.append(scores['rouge1'].fmeasure)\n",
    "                all_q_gens_scores_1r.append(scores['rouge1'].recall)\n",
    "                all_q_gens_scores_Lf.append(scores['rougeL'].fmeasure)\n",
    "                all_q_gens_scores_Lr.append(scores['rougeL'].recall)\n",
    "\n",
    "            max_scores_1f.append(max(all_q_gens_scores_1f))\n",
    "            max_scores_1r.append(max(all_q_gens_scores_1r))\n",
    "            max_scores_Lf.append(max(all_q_gens_scores_Lf))\n",
    "            max_scores_Lr.append(max(all_q_gens_scores_Lr))\n",
    "\n",
    "# print(max_scores)\n",
    "print(np.mean(max_scores_1f), 'R1-F')\n",
    "print(np.mean(max_scores_1r), 'R1-R')\n",
    "print(np.mean(max_scores_Lf), 'RL-F')\n",
    "print(np.mean(max_scores_Lr), 'RL-R')\n",
    "\n",
    "print(np.mean(n_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
